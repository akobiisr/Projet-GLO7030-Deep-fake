{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copie de fake-image-detection-project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-Yly4fQhA-LO"
      },
      "source": [
        "# DEEPFAKE DETECTION PROJECT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CsuOcZk9A-Li",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zd--KllNPNy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install poutyne\n",
        "!pip3 install efficientnet-pytorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hjhgKOh9A-Lu",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks/deepfake/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tWguqkOXA-Lz",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from functools import partial\n",
        "\n",
        "from blazeface import BlazeFace\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "\n",
        "from poutyne.framework import Model, ModelCheckpoint, BestModelRestore, CSVLogger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooXMV8TJxVE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U6rFYG20A-L9"
      },
      "source": [
        "## Datasets Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8zrO7GjUA-L_",
        "colab": {}
      },
      "source": [
        "FFPP_SRC='/content/gdrive/My Drive/Colab Notebooks/deepfake/datasets'\n",
        "VIDEODF_SRC = '/content/gdrive/My Drive/Colab Notebooks/deepfake/datasets/ffpp_videos.pkl'\n",
        "\n",
        "BLAZEFACE_WEIGHTS = \"/content/gdrive/My Drive/Colab Notebooks/deepfake/blazeface/blazeface.pth\"\n",
        "BLAZEFACE_ANCHORS = \"/content/gdrive/My Drive/Colab Notebooks/deepfake/blazeface/anchors.npy\"\n",
        "\n",
        "FACES_DST = '/content/gdrive/My Drive/Colab Notebooks/deepfake/datasets/extract_faces'\n",
        "FACESDF_DST = '/content/gdrive/My Drive/Colab Notebooks/deepfake/datasets/extract_faces/ffpp_faces.pkl'\n",
        "CHECKPOINT_DST = '/content/gdrive/My Drive/Colab Notebooks/deepfake/datasets/extract_faces/checkpoint'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hkMswaAHA-MF"
      },
      "source": [
        "### FF++ Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wkz8jXImA-MH",
        "colab": {}
      },
      "source": [
        "def preprocess_ffpp(source_dir, video_dataset_path):\n",
        "    \"\"\"\n",
        "    Preprocessing video dataset\n",
        "    :param source_dir : Source dir\n",
        "    :param video_dataset : Path to save the videos DataFrame\n",
        "    \"\"\"\n",
        "    source_dir = Path(source_dir)\n",
        "    video_dataset_path = Path(video_dataset_path)\n",
        "    \n",
        "    # Create ouput folder (if doesn't exist)\n",
        "    video_dataset_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ## DataFrame\n",
        "    if video_dataset_path.exists():\n",
        "        print('Loading video DataFrame')\n",
        "        df_videos = pd.read_pickle(video_dataset_path)\n",
        "    else:\n",
        "        print('Creating video DataFrame')\n",
        "        df_videos = pd.DataFrame({'path': [f.relative_to(source_dir) for f in source_dir.rglob('*.mp4')]})\n",
        "        \n",
        "        df_videos['label'] = df_videos['path'].map(\n",
        "            lambda x: 1 if x.parts[0] == 'manipulated_sequences' else 0)  # 1 if fake, otherwise 0\n",
        "        \n",
        "        source = df_videos['path'].map(lambda x: x.parts[1]).astype('category')\n",
        "        df_videos['name'] = df_videos['path'].map(lambda x: x.with_suffix('').parts[-1])\n",
        "        df_videos['path'] = df_videos['path'].map(lambda x: str(x))\n",
        "\n",
        "        df_videos['original'] = -1 * np.ones(len(df_videos), dtype=np.int16)\n",
        "        df_videos.loc[(df_videos['label'] == 1) & (source != 'DeepFakeDetection'), 'original'] = \\\n",
        "            df_videos[(df_videos['label'] == 1) & (source != 'DeepFakeDetection')]['name'].map(\n",
        "                lambda x: df_videos.index[np.flatnonzero(df_videos['name'] == x.split('_')[0])[0]]\n",
        "            )\n",
        "        df_videos.loc[(df_videos['label'] == 1) & (source == 'DeepFakeDetection'), 'original'] = \\\n",
        "            df_videos[(df_videos['label'] == 1) & (source == 'DeepFakeDetection')]['name'].map(\n",
        "                lambda x: df_videos.index[\n",
        "                    np.flatnonzero(df_videos['name'] == x.split('_')[0] + '__' + x.split('__')[1])[0]]\n",
        "            )\n",
        "\n",
        "        print('Saving video DataFrame to {}'.format(video_dataset_path))\n",
        "        df_videos.to_pickle(str(video_dataset_path))\n",
        "\n",
        "    print('Real videos: {:d}'.format(sum(df_videos['label'] == 0)))\n",
        "    print('Fake videos: {:d}'.format(sum(df_videos['label'] == 1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8YjT22kUA-MN"
      },
      "source": [
        "### Extract faces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J7r42lQjA-MP",
        "colab": {}
      },
      "source": [
        "def extract_faces_on_video(video_df, source_dir, faces_dir, checkpoint_dir, blazeface, num_frames, face_size=224, margin=0.25):\n",
        "    video_idx, video_df = video_df\n",
        "    faces_checkpoint_path = Path(checkpoint_dir).joinpath(video_df['path'].split('.')[0] + '_faces.pkl')\n",
        "    if faces_checkpoint_path.exists():\n",
        "        faces = pd.read_pickle(faces_checkpoint_path)\n",
        "        return faces\n",
        "    \n",
        "    reader = cv2.VideoCapture(str(Path(source_dir).joinpath(video_df['path'])))\n",
        "    frame_count = int(reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_idx = np.unique(np.linspace(0, frame_count - 1, num_frames, dtype=np.int))\n",
        "    # Get the frames choosen\n",
        "    frames, idx = [], 0\n",
        "    while reader.grab():\n",
        "        if idx in frame_idx:\n",
        "            ret, frame = reader.retrieve()\n",
        "            if not ret or frame is None:\n",
        "                print(\"Error retrieving frame %d from movie %s\" % (frame_idx, path))\n",
        "                break\n",
        "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        idx += 1\n",
        "    frames = np.stack(frames)\n",
        "\n",
        "    target_w, target_h = blazeface.input_size\n",
        "    # For each frame, get some tiles with target_w x target_h as size\n",
        "    num_frames, height, width, _ = frames.shape\n",
        "\n",
        "    split_size = min(height, width, 720)\n",
        "    x_step = (width - split_size) // 2\n",
        "    y_step = (height - split_size) // 2\n",
        "    num_h = (height - split_size) // y_step + 1 if y_step > 0 else 1\n",
        "    num_w = (width - split_size) // x_step + 1 if x_step > 0 else 1\n",
        "\n",
        "    tiles = np.zeros((num_frames * num_h * num_w, target_h, target_w, 3), dtype=np.uint8)\n",
        "    i = 0\n",
        "    for f in range(num_frames):\n",
        "        y = 0\n",
        "        for _ in range(num_h):\n",
        "            x = 0\n",
        "            for __ in range(num_w):\n",
        "                crop = frames[f, y:y + split_size, x:x + split_size, :]\n",
        "                tiles[i] = cv2.resize(crop, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
        "                x += x_step\n",
        "                i += 1\n",
        "            y += y_step\n",
        "\n",
        "    # Run the face detector. The result is a list of PyTorch tensors\n",
        "    detections = blazeface.predict_on_batch(tiles, apply_nms=False)\n",
        "    # Convert the detections from 128x128 back to the original frame size\n",
        "    for i in range(len(detections)):\n",
        "        # ymin, xmin, ymax, xmax\n",
        "        for k in range(2):\n",
        "            detections[i][:, k * 2] = (detections[i][:, k * 2] * target_h) * split_size / target_h\n",
        "            detections[i][:, k * 2 + 1] = (detections[i][:, k * 2 + 1] * target_w) * split_size / target_w\n",
        "\n",
        "    # Because we have several tiles for each frame, combine the predictions from these tiles.\n",
        "    combined_detections = []\n",
        "    i = 0\n",
        "    for f in range(num_frames):\n",
        "        detections_for_frame = []\n",
        "        y = 0\n",
        "        for _ in range(num_h):\n",
        "            x = 0\n",
        "            for __ in range(num_w):\n",
        "                # Adjust the coordinates based on the split positions.\n",
        "                if detections[i].shape[0] > 0:\n",
        "                    for k in range(2):\n",
        "                        detections[i][:, k * 2] += y\n",
        "                        detections[i][:, k * 2 + 1] += x\n",
        "                \n",
        "                detections_for_frame.append(detections[i])\n",
        "                x += x_step\n",
        "                i += 1\n",
        "            y += y_step\n",
        "            \n",
        "        combined_detections.append(torch.cat(detections_for_frame))\n",
        "    if len(combined_detections) == 0:\n",
        "        return None\n",
        "    detections = blazeface.nms(combined_detections)\n",
        "    # Crop the faces out of the original frame.\n",
        "    faces = []\n",
        "    for i in range(len(detections)):\n",
        "        offset = torch.round(margin * (detections[i][:, 2] - detections[i][:, 0])) # margin 0.2\n",
        "        detections[i][:, 0] = torch.clamp(detections[i][:, 0] - offset * 2, min=0)  # ymin\n",
        "        detections[i][:, 1] = torch.clamp(detections[i][:, 1] - offset, min=0)  # xmin\n",
        "        detections[i][:, 2] = torch.clamp(detections[i][:, 2] + offset, max=height)  # ymax\n",
        "        detections[i][:, 3] = torch.clamp(detections[i][:, 3] + offset, max=width)  # xmax\n",
        "        \n",
        "        # Get the first best scored face\n",
        "        score, face, detection = 0, None, None\n",
        "        for j in range(len(detections[i])):\n",
        "            if score < detections[i][j][16].cpu():\n",
        "                detection = detections[i][j].cpu()\n",
        "                ymin, xmin, ymax, xmax = detection[:4].cpu().numpy().astype(np.int)\n",
        "                face = frames[i][ymin:ymax, xmin:xmax, :]\n",
        "                score = detection[16]\n",
        "                break\n",
        "        if face is not None:\n",
        "            image = Image.fromarray(face)\n",
        "            # Crop the image to face_size x face_size\n",
        "            top, left, bottom, right = detection[:4].cpu().numpy().astype(np.int)\n",
        "            x_ctr = (left + right) // 2\n",
        "            y_ctr = (top + bottom) // 2\n",
        "            new_top = max(y_ctr - face_size // 2, 0)\n",
        "            new_bottom = min(new_top + face_size, height)\n",
        "            new_left = max(x_ctr - face_size // 2, 0)\n",
        "            new_right = min(new_left + face_size, width)\n",
        "            image.crop([new_left, new_top, new_right, new_bottom])\n",
        "            # Save image\n",
        "            face_path = Path(faces_dir).joinpath(video_df['path']).joinpath('frame_{}.jpg'.format(frame_idx[i]))\n",
        "            face_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            image.save(face_path)\n",
        "            \n",
        "            faces.append({\n",
        "                'path': str(Path(video_df['path']).joinpath('frame_{}.jpg'.format(frame_idx[i]))),\n",
        "                'label': video_df['label'],\n",
        "                'video': video_idx,\n",
        "                'original': video_df['original'],\n",
        "                'frame_index': frame_idx[i],\n",
        "                'score': float(score.numpy()),\n",
        "                'detection': detection[:4].cpu().numpy().astype(np.int)\n",
        "            })\n",
        "        # Save checkpoint\n",
        "        faces_checkpoint_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "        pd.DataFrame(faces).to_pickle(str(faces_checkpoint_path))\n",
        "        \n",
        "    return faces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qIy8YsWJA-MU",
        "colab": {}
      },
      "source": [
        "def extract_faces(source_dir, videos_df, faces_dir, faces_df, checkpoint_dir, \n",
        "                  frames_per_video=15, batch_size=16, face_size=224, thread_num=4):\n",
        "    \n",
        "    if Path(faces_df).exists():\n",
        "        df_faces = pd.read_pickle(faces_df)\n",
        "        print('We got {} faces'.format(len(df_faces)))\n",
        "        print('Faces DataFrame Loaded')\n",
        "        return df_faces\n",
        "    \n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    \n",
        "    print('Loading video DataFrame')\n",
        "    df_videos = pd.read_pickle(videos_df)\n",
        "    \n",
        "    print('Loading Blazeface model')\n",
        "    blazeface_net = BlazeFace().to(device)\n",
        "    blazeface_net.load_weights(BLAZEFACE_WEIGHTS)\n",
        "    blazeface_net.load_anchors(BLAZEFACE_ANCHORS)    \n",
        "    blazeface_net.min_score_thresh = 0.8\n",
        "    \n",
        "    ## Face extraction\n",
        "    with ThreadPoolExecutor(thread_num) as pool:\n",
        "        for batch_idx0 in tqdm(np.arange(start=0, stop=len(df_videos), step=batch_size),\n",
        "                               desc='Extracting faces'):\n",
        "            list(pool.map(partial(extract_faces_on_video,\n",
        "                          source_dir=source_dir,\n",
        "                          faces_dir=faces_dir,\n",
        "                          checkpoint_dir=checkpoint_dir,\n",
        "                          blazeface=blazeface_net,\n",
        "                          num_frames=frames_per_video,\n",
        "                          face_size=face_size,\n",
        "                          ),\n",
        "                          df_videos.iloc[batch_idx0:batch_idx0 + batch_size].iterrows()))\n",
        "    \n",
        "    faces_dataset = []\n",
        "    for _, df in tqdm(df_videos.iterrows(), total=len(df_videos), desc='Collecting faces results'):\n",
        "        face_checkpoint = Path(checkpoint_dir).joinpath(df['path'].split('.')[0] + '_faces.pkl')\n",
        "        if face_checkpoint.exists():\n",
        "            faces_dataset.append(pd.read_pickle(str(face_checkpoint)))\n",
        "    df_faces = pd.concat(faces_dataset, axis=0)\n",
        "    df_faces.to_pickle(faces_df)\n",
        "    print('We got {} faces'.format(len(df_faces)))\n",
        "    print('Completed!')\n",
        "    return df_faces"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "50I8DTgAA-MX",
        "colab": {}
      },
      "source": [
        "# preprocess ff++ data\n",
        "preprocess_ffpp(FFPP_SRC, VIDEODF_SRC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wy0Y5AwtA-Mb",
        "colab": {}
      },
      "source": [
        "# Run extraction\n",
        "df_faces = extract_faces(FFPP_SRC, VIDEODF_SRC, FACES_DST, FACESDF_DST,  CHECKPOINT_DST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mIzeeGplA-NQ"
      },
      "source": [
        "## Dataset processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6_y-9WSIA-NR",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hb3FfzYeA-NU"
      },
      "source": [
        "### Split dataset on original video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rLAoN8D1A-NU",
        "colab": {}
      },
      "source": [
        "def make_splits(df_faces, train_ratio=0.7, val_ratio=0.15):\n",
        "    random_original_videos = np.random.permutation(df_faces[(df_faces['label'] == 0)]['video'].unique())\n",
        "    train_num = int(len(random_original_videos) * train_ratio)\n",
        "    val_num = int(len(random_original_videos) * val_ratio)\n",
        "    train_original = random_original_videos[:train_num]\n",
        "    val_original = random_original_videos[train_num: train_num + val_num]\n",
        "    test_original = random_original_videos[train_num + val_num:]\n",
        "    \n",
        "    df_train = pd.concat([df_faces[df_faces['original'].isin(train_original)], df_faces[df_faces['video'].isin(train_original)]], ignore_index=True)\n",
        "    df_val = pd.concat([df_faces[df_faces['original'].isin(val_original)], df_faces[df_faces['video'].isin(val_original)]], ignore_index=True)\n",
        "    df_test = pd.concat([df_faces[df_faces['original'].isin(test_original)], df_faces[df_faces['video'].isin(test_original)]], ignore_index=True)\n",
        "    \n",
        "    return df_train, df_val, df_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtLSibzXCXIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_faces = pd.read_pickle(FACESDF_DST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FdyPSuBChgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_faces.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KLy4qAuQA-NZ",
        "colab": {}
      },
      "source": [
        "df_train, df_val, df_test = make_splits(df_faces)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4pVM8ty6A-Nf"
      },
      "source": [
        "### Transforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RR0i7TC5A-Nf",
        "colab": {}
      },
      "source": [
        "pre_trained_mean, pre_trained_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomAffine(degrees=40, scale=(.9, 1.1), shear=0),\n",
        "    transforms.RandomPerspective(distortion_scale=0.2),\n",
        "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(scale=(0.02, 0.16), ratio=(0.3, 1.6)),\n",
        "    transforms.Normalize(mean=pre_trained_mean, std=pre_trained_std)\n",
        "])\n",
        "\n",
        "val_tranform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=pre_trained_mean, std=pre_trained_std)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f8iG1vBAA-Ni"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YB92wiiA-Nj",
        "colab": {}
      },
      "source": [
        "class FFPPDataset(Dataset):\n",
        "    def __init__(self, df_faces, faces_dir=FACES_DST, transform=None):\n",
        "        super().__init__()\n",
        "        self.faces_dir = Path(faces_dir)\n",
        "        self.data, self.targets = df_faces['path'], df_faces['label']\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        img_path, target = self.data[index], self.targets[index]\n",
        "        target = np.array([target,]).astype(np.float32)\n",
        "        # read the image\n",
        "        img = Image.open(str(self.faces_dir.joinpath(img_path)))\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rxBdI_boA-Nm",
        "colab": {}
      },
      "source": [
        "train_dataset = FFPPDataset(df_train, transform=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=24, pin_memory=True)\n",
        "\n",
        "valid_dataset = FFPPDataset(df_val, transform=val_tranform)\n",
        "val_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, num_workers=24, pin_memory=True)\n",
        "\n",
        "test_dataset = FFPPDataset(df_test, transform=val_tranform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=24, pin_memory=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7swTRXvMbld",
        "colab_type": "text"
      },
      "source": [
        "## Entraîner le réseau EfficientNet\n",
        "\n",
        "Modèle pré-entraîner nommé : efficientnet-b4\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVLsLQ3N1Ha5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training configuration\n",
        "# number of epochs to train for\n",
        "num_epochs = 10\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# loss_function\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NTgHso71zkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, model_name):\n",
        "  model_path = '/content/gdrive/My Drive/Colab Notebooks/deepfake/models/' + model_name + '/best_epoch_{epoch}.ckpt'\n",
        "  Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "  logs_file = '/content/gdrive/My Drive/Colab Notebooks/deepfake/logs/' + model_name + '/log.tsv'\n",
        "  Path(logs_file).parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  callbacks = [\n",
        "    # Save the weights in a new file when the current model is better than all previous models.\n",
        "    ModelCheckpoint(model_path, monitor='val_bin_acc', mode='max', save_best_only=True, restore_best=True, verbose=True, temporary_filename=model_name + '_best_epoch.ckpt.tmp'),\n",
        "    \n",
        "    # Save the losses and accuracies for each epoch in a TSV.\n",
        "    CSVLogger(logs_file, separator='\\t'),\n",
        "  ]\n",
        "\n",
        "  params = (p for p in net.parameters() if p.requires_grad)\n",
        "  optimizer = optim.Adam(params, lr=learning_rate, weight_decay=0.000001)\n",
        "\n",
        "  model = Model(net, optimizer, criterion, batch_metrics=['bin_acc'])\n",
        "  model.to(device)\n",
        "\n",
        "  if Path(logs_file).exists():\n",
        "    logs = pd.read_csv(logs_file, sep='\\t')\n",
        "    epochs = list(logs['epoch'])\n",
        "    if len(epochs) != 0:\n",
        "      best_epoch_idx = logs['val_bin_acc'].idxmax()\n",
        "      best_epoch = int(logs.loc[best_epoch_idx]['epoch'])\n",
        "      model.load_weights(model_path.format(epoch=best_epoch))\n",
        "      if num_epochs not in epochs:\n",
        "        # Train\n",
        "        model.fit_generator(train_loader, val_loader, epochs=num_epochs, initial_epoch=epochs[-1], callbacks=callbacks)\n",
        "    else:\n",
        "      model.fit_generator(train_loader, val_loader, epochs=num_epochs, callbacks=callbacks)\n",
        "  else:\n",
        "    model.fit_generator(train_loader, val_loader, epochs=num_epochs, callbacks=callbacks)\n",
        "\n",
        "  logs = pd.read_csv(logs_file, sep='\\t')\n",
        "\n",
        "  best_epoch_idx = logs['val_bin_acc'].idxmax()\n",
        "  best_epoch = int(logs.loc[best_epoch_idx]['epoch'])\n",
        "  print(\"Best epoch: %d\" % best_epoch)\n",
        "\n",
        "  metrics = ['loss', 'val_loss']\n",
        "  plt.plot(logs['epoch'], logs[metrics])\n",
        "  plt.legend(metrics)\n",
        "  plt.title('Loss')\n",
        "  plt.show()\n",
        "\n",
        "  metrics = ['bin_acc', 'val_bin_acc']\n",
        "  plt.plot(logs['epoch'], logs[metrics])\n",
        "  plt.legend(metrics)\n",
        "  plt.title('Accuracy')\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKbzkBCb-vQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader):\n",
        "  test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "  print('Test:\\n\\tLoss: {}\\n\\tAccuracy: {}'.format(test_loss, test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc8EM1TS1FyQ",
        "colab_type": "text"
      },
      "source": [
        "### Freezant tous les paramètres de convolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkzbm95FFODY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "efficientnet_a = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "efficientnet_a._fc = nn.Linear(efficientnet_a._conv_head.out_channels, 1)\n",
        "\n",
        "for name, param in efficientnet_a.named_parameters():\n",
        "  if not name.startswith('_fc'):\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_a = train(efficientnet_a, 'efficientnet_a')\n",
        "test(model_a, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5_FQ-nrQAYz",
        "colab_type": "text"
      },
      "source": [
        "### En réentraînant quelques blocs\n",
        "\n",
        "Nous allons réentraîner les 8 derniers blocs avec les deux dernières couches (une couche de convolution et le classificateur)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGvhRsFfULPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "efficientnet_b = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "efficientnet_b._fc = nn.Linear(efficientnet_b._conv_head.out_channels, 1)\n",
        "\n",
        "names = [name for name, _ in list(efficientnet_b.named_parameters())[-109:]]\n",
        "for name, param in efficientnet_b.named_parameters():\n",
        "  if name not in names:\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_b = train(efficientnet_b, 'efficientnet_b')\n",
        "test(model_b, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq3kPtqzmEsJ",
        "colab_type": "text"
      },
      "source": [
        "Nous allons réentraîner les 16 derniers blocs avec les deux dernières couches (une couche de convolution et le classificateur)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2phbJ577fhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "efficientnet_c = EfficientNet.from_pretrained('efficientnet-b4')\n",
        "efficientnet_c._fc = nn.Linear(efficientnet_c._conv_head.out_channels, 1)\n",
        "\n",
        "names = [name for name, _ in list(efficientnet_c.named_parameters())[-213:]]\n",
        "for name, param in efficientnet_c.named_parameters():\n",
        "  if name not in names:\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_c = train(efficientnet_c, 'efficientnet_c')\n",
        "test(model_c, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeeCiYiSXIqT",
        "colab_type": "text"
      },
      "source": [
        "# Simple mécanisme d'attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9SSJEbFXSe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EfficientNetSimpleAtt(EfficientNet):\n",
        "    def init_att(self, width: int):\n",
        "      \"\"\"\n",
        "        Initialize attention\n",
        "        :param depth: attention width\n",
        "        :return:\n",
        "      \"\"\"\n",
        "        self.att_block_idx = 9\n",
        "        if width == 0:\n",
        "            self.attconv = nn.Conv2d(kernel_size=1, in_channels=56, out_channels=1)\n",
        "        else:\n",
        "            attconv_layers = []\n",
        "            for i in range(width):\n",
        "                attconv_layers.append(\n",
        "                    ('conv{:d}'.format(i), nn.Conv2d(kernel_size=3, padding=1, in_channels=56, out_channels=56)))\n",
        "                attconv_layers.append(\n",
        "                    ('relu{:d}'.format(i), nn.ReLU(inplace=True)))\n",
        "            attconv_layers.append(('conv_out', nn.Conv2d(kernel_size=1, in_channels=56, out_channels=1)))\n",
        "            self.attconv = nn.Sequential(OrderedDict(attconv_layers))\n",
        "\n",
        "    def get_attention(self, inputs):\n",
        "        att = None\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "            if idx == self.att_block_idx:\n",
        "                att = torch.sigmoid(self.attconv(x))\n",
        "                break\n",
        "\n",
        "        return att\n",
        "\n",
        "    def extract_features(self, inputs):\n",
        "        # Stem\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "            if idx == self.att_block_idx:\n",
        "                att = torch.sigmoid(self.attconv(x))\n",
        "                x = x * att\n",
        "\n",
        "        # Head\n",
        "        x = self._swish(self._bn1(self._conv_head(x)))\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3eKWWt1cohQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "efficientnet_e = EfficientNetSimpleAtt.from_pretrained('efficientnet-b4')\n",
        "efficientnet_e._fc = nn.Linear(efficientnet_e._conv_head.out_channels, 1)\n",
        "efficientnet_e.init_att(width)\n",
        "\n",
        "names = [name for name, _ in list(efficientnet_e.named_parameters())[-221:]]\n",
        "for name, param in efficientnet_e.named_parameters():\n",
        "  if name not in names:\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_e = train(efficientnet_e, 'efficientnet_e')\n",
        "test(model_e, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tQJWTsnhWtk",
        "colab_type": "text"
      },
      "source": [
        "## Attention spatiale : Spatial Transfomer Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhHp-pz8qX1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EfficientNetAtt(EfficientNet):\n",
        "    def __init__(self, blocks_args=None, global_params=None):\n",
        "        super().__init__(blocks_args, global_params)\n",
        "        \n",
        "    def init_spatial_transformer(self):\n",
        "        self.block_idx = 20\n",
        "        \n",
        "        # Spatial transformer localization-network\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(160, 160, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(160, 80, kernel_size=3),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "        \n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(80 * 1 * 1, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "        \n",
        "        # Initialize the weights/bias with identity transformation\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "    \n",
        "    def stn(self, inputs):\n",
        "        inputs = inputs.clone()\n",
        "        xs = self.localization(inputs)\n",
        "        xs = xs.view(-1, 80 * 1 * 1)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, inputs.size(), align_corners=False)\n",
        "        x = F.grid_sample(inputs, grid, align_corners=False)\n",
        "\n",
        "        return x\n",
        "    \n",
        "    def extract_features(self, inputs):\n",
        "        \"\"\" Returns output of the final convolution layer \"\"\"\n",
        "\n",
        "        # Stem\n",
        "        x = self._swish(self._bn0(self._conv_stem(inputs)))\n",
        "\n",
        "        # Blocks\n",
        "        for idx, block in enumerate(self._blocks):\n",
        "            drop_connect_rate = self._global_params.drop_connect_rate\n",
        "            if drop_connect_rate:\n",
        "                drop_connect_rate *= float(idx) / len(self._blocks)\n",
        "            x = block(x, drop_connect_rate=drop_connect_rate)\n",
        "            if idx == self.block_idx:\n",
        "                x *= self.stn(x)\n",
        "\n",
        "        # Head\n",
        "        x = self._swish(self._bn1(self._conv_head(x)))\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCefQSQCiKKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "efficientnet_d = EfficientNetAtt.from_pretrained('efficientnet-b4')\n",
        "efficientnet_d._fc = nn.Linear(efficientnet_d._conv_head.out_channels, 1)\n",
        "efficientnet_d.init_spatial_transformer()\n",
        "\n",
        "names = [name for name, _ in list(efficientnet_d.named_parameters())[-221:]]\n",
        "for name, param in efficientnet_d.named_parameters():\n",
        "  if name not in names:\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_d = train(efficientnet_d, 'efficientnet_d')\n",
        "test(model_d, test_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}